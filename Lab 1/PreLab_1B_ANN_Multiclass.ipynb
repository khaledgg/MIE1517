{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6x62xMrvYsy"
   },
   "source": [
    "## PreLab 1B - Multi-Class ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1Mmucb8tjca"
   },
   "source": [
    "### Motivation\n",
    "\n",
    "In the lectures we have seen how Multi-Class Artificial Neural Networks (ANNs) can be trained and tested to classify handwritten digits. To truly appreciate this capability we will see if we can apply our ANN model to work with new images of hand written digits. \n",
    "\n",
    "For example given a blank white sheet of paper with a hand written digit on it, how could we use an ANN model to correctly identify the digit. That will be our objective in this tutorial and along the way you will learn a few tricks to speedup the training of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tBDXHq2O21o"
   },
   "source": [
    "### MNIST Multi-Class Classification\n",
    "\n",
    "To begin, let us load the MNIST dataset and divide it for training and validation. Note that we have left some of the 60,000 samples for final testing of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "5ijcHOPvjycm",
    "outputId": "bccf92d0-36ae-4447-9b1f-4509bf53e64b"
   },
   "outputs": [],
   "source": [
    "# obtain data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_data = list(mnist_data)\n",
    "mnist_train = mnist_data[:4096]\n",
    "mnist_val   = mnist_data[4096:5120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yquv5GYb0N31"
   },
   "source": [
    "### Multi-Class ANN Architecture\n",
    "In this example we will be using a 3-layer ANN with ReLU activation functions applied on the first and second hidden layers. The softmax activation will be used for outputting class probabilities and is not included in the architecture setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "tRLaDEG9j2p5",
    "outputId": "1d9db730-8375-45aa-ab68-8f75d5319427"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import torch.optim as optim #for gradient descent\n",
    "\n",
    "torch.manual_seed(1) # set the random seed\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(28 * 28, 50)\n",
    "        self.layer2 = nn.Linear(50, 20)\n",
    "        self.layer3 = nn.Linear(20, 10)\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        activation1 = F.relu(self.layer1(flattened))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "        return output\n",
    "\n",
    "model = MNISTClassifier()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SiuzX-Z1BlZ"
   },
   "source": [
    "### Function to Obtain Accuracy\n",
    "The get_accuracy function is used to compute the accuracy on training or validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yKekyWFBkBQW",
    "outputId": "a557444c-3a29-4647-eb75-3f5800210178"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, train=False):\n",
    "    if train:\n",
    "        data = mnist_train\n",
    "    else:\n",
    "        data = mnist_val\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=64):\n",
    "        output = model(imgs)\n",
    "        #select index with maximum prediction score\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total\n",
    "  \n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmPvsefY1nbJ"
   },
   "source": [
    "### Function to perform Training and Validation\n",
    "The train function puts everything together. You can provide arguments to adjust the batch size and number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xhg5emvhO2OU",
    "outputId": "530954dc-9201-449e-a917-9aedf158abe3"
   },
   "outputs": [],
   "source": [
    "def train(model, data, batch_size=64, num_epochs=1):\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "\n",
    "    # training\n",
    "    n = 0 # the number of iterations\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs, labels in iter(train_loader):\n",
    "            out = model(imgs)             # forward pass\n",
    "            loss = criterion(out, labels) # compute the total loss\n",
    "            loss.backward()               # backward pass (compute parameter updates)\n",
    "            optimizer.step()              # make the updates for each parameter\n",
    "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
    "\n",
    "            # save the current training information\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
    "            train_acc.append(get_accuracy(model, train=True)) # compute training accuracy \n",
    "            val_acc.append(get_accuracy(model, train=False))  # compute validation accuracy\n",
    "            n += 1\n",
    "\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, train_acc, label=\"Train\")\n",
    "    plt.plot(iters, val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
    "    \n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiMfDKEi2efq"
   },
   "source": [
    "### (optional) Sanity Check\n",
    "Verify that the model is able to overfit on a single batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "KaFseq7qkHL6",
    "outputId": "2efb3f9f-78db-4b3a-eed6-9381229a8460"
   },
   "outputs": [],
   "source": [
    "#overfitting the model (sanity check)\n",
    "debug_data = mnist_train[:64]\n",
    "model = MNISTClassifier()\n",
    "train(model, debug_data, num_epochs=500)\n",
    "\n",
    "#obtain accuracy on 64 samples\n",
    "correct = 0\n",
    "total = 0\n",
    "for imgs, labels in torch.utils.data.DataLoader(debug_data, batch_size=64):\n",
    "    output = model(imgs)\n",
    "    #select index with maximum prediction score\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    total += imgs.shape[0]\n",
    "print('Accuracy on batch of 64: ', correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V7lwJ8c7mCP"
   },
   "source": [
    "Note the Final Training and Validation accuracy is obtained on the full training data and validation data. It does not reflect the performance on the 64 samples that were overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3iEpnAp2vxG"
   },
   "source": [
    "### Run Training and Validation\n",
    "Now that we've validated that our model can overfit a relatively small amount of training data (i.e. 64 samples), we can proceed to train our model on all of the training data. \n",
    "\n",
    "We will be training our model over 5 epochs (how many training iterations is that?) to ensure that we can complete this tutorial in a reasonable time. In your free time you are welcome to explore the model accuracy as you increase the number of epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "sS4CesVbk4M_",
    "outputId": "e28ab00a-58c1-45f5-df43-0134fcdea1ff"
   },
   "outputs": [],
   "source": [
    "#proper model\n",
    "model = MNISTClassifier()\n",
    "train(model, mnist_train, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JdgEbrk247X"
   },
   "source": [
    "### (optional) Additional Training\n",
    "At this stage we can consider adjusting our model architecture: \n",
    "- number of hidden layers, \n",
    "- hidden units, \n",
    "- activation functions, \n",
    "- optimizers, \n",
    "- learning rate, \n",
    "- momentum, \n",
    "- batch size,\n",
    "- training iterations, \n",
    "\n",
    "to evaluate the performance of our ANN model. Can we do better?\n",
    "\n",
    "**Tip:** Once you have searched through the hyperparameters and obtained model parameters that work reasonably well. You may want to save your model so that you don't have to retrain the model next time you open the Google Colab file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbHuVE-l-HbP"
   },
   "outputs": [],
   "source": [
    "# save the model for next time\n",
    "torch.save(model.state_dict(), \"saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKVU13bXlHgk"
   },
   "source": [
    "### Test one image\n",
    "At this point we have trained our model and observed accuracy scores on the training data and validation data. We haven't really looked at the data. For the next stage of the tutorial we will try to understand what the data looks like and consider what is required to classify new images obtained from the internet, or even our cell phone camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "znrJoUqNlWdu",
    "outputId": "ecf0aa1f-9c5f-4a99-c04e-e4cdea0c3a03"
   },
   "outputs": [],
   "source": [
    "#laod new image for testing\n",
    "mnist_sample = mnist_data[19120] #samples with indices > 5120 can be used for testing\n",
    "img, label = list(mnist_sample) #obtain a single image and label\n",
    "\n",
    "#plot sample image\n",
    "print('image dimensions: ', img.shape)\n",
    "plt.imshow(img.view(-1,28)) #make image 28 x 28 (not 1 x 28 x 28 as required by model)\n",
    "\n",
    "#test new image\n",
    "out = model(img)\n",
    "prob = F.softmax(out, dim=1)\n",
    "print('output dimensions: ', out.shape)\n",
    "print('output probabilities: ', prob, 'sum: ', torch.sum(prob))\n",
    "\n",
    "#print max index and compare with label\n",
    "print('output: ', prob.max(1, keepdim=True)[1].item(), 'with a probability of', prob.max(1, keepdim=True)[0].item())\n",
    "print('label: ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFDGZpKpN3En"
   },
   "source": [
    "### Exploring the MNIST data\n",
    "Before we can load new data for testing we should understand what preprocessing went into making the training data. We will explore:\n",
    "- Data Type\n",
    "- Data Dimensions\n",
    "- Data Normalization\n",
    "- Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "B10pYF2aPJNS",
    "outputId": "584e47e9-10c5-49dd-c90e-4450ff9ca9a2"
   },
   "outputs": [],
   "source": [
    "# data type\n",
    "print(img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "SkADH8L9O5QN",
    "outputId": "4f0eb747-e943-4870-eb27-618e4d93f6e9"
   },
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "tMyDOy61gp7X",
    "outputId": "3ae52226-c1c3-4f1b-deea-b37cb2118320"
   },
   "outputs": [],
   "source": [
    "# max and min values\n",
    "print('min val:', torch.min(img).item(), '  max val:', torch.max(img).item())\n",
    "\n",
    "#histogram of values\n",
    "plt.hist(img.view(-1,28*28), bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wowo4MYmQQh_"
   },
   "source": [
    "Now that we know a little bit about our data we should be able to generate new samples of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKY4i99TgcxZ"
   },
   "source": [
    "### Load New Image\n",
    "In the Google Colab environment there are a number of ways to load data samples. If you are using Chrome or Chromium you should be able to just load the data into the workspace using the following code. Alternatively, you can always read an image posted online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 41,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "FXtCbTJQghSi",
    "outputId": "c3fa4210-fb7b-4058-9987-19b75b07265d"
   },
   "outputs": [],
   "source": [
    "#optional for laoding data from a file\n",
    "from google.colab import files\n",
    "img_new = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY3dlM69iRqY"
   },
   "outputs": [],
   "source": [
    "#loading data from the internet\n",
    "img_new = plt.imread('https://www.researchgate.net/profile/Hariton_Costin/publication/311806756/figure/fig1/AS:542753920229376@1506414026147/Sample-of-the-MNIST-dataset-of-handwritten-digits.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40h4pHsrROkL"
   },
   "source": [
    "### Examine the image\n",
    "How does the image we loaded differ from the one in the MNIST dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "6467ozkTRa3M",
    "outputId": "9a8d3a24-69e9-409e-da06-74ad62ad6218"
   },
   "outputs": [],
   "source": [
    "print('Image Dimensions:', img_new.shape)\n",
    "print('Image Type: ', img_new.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "jYvse6Kij1BY",
    "outputId": "fd7fc059-7a36-47f7-edd8-777a0d393c16"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert from colour to grayscale\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "  \n",
    "img_gray = rgb2gray(img_new)\n",
    "\n",
    "plt.title(\"New Image\")\n",
    "plt.imshow(img_gray)\n",
    "plt.show()\n",
    "\n",
    "# compare to original MNIST image\n",
    "plt.title(\"Original MNIST\")\n",
    "plt.imshow(img.view(-1,28)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOfNJL-ESJ33"
   },
   "source": [
    "Notice that the image colours are inverated? How will this affect our classification on new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFCMy36-SbI9"
   },
   "source": [
    "### Inverting Colours\n",
    "One option is to just load an image with the colours inverted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "X65Xscysllkq",
    "outputId": "0b24b265-9644-441a-f556-9cd10985e9c4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load an image with black and white matching the MNIST data\n",
    "img_new = plt.imread('https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT5ZR8ImWkYVd2FRMZgUvCdNkHx0uKjjSAtTEJ0U-x0SPWQFxqnbg')\n",
    "print('Image Dimensions', img_new.shape)\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.144])\n",
    "  \n",
    "img_gray = rgb2gray(img_new)\n",
    "\n",
    "plt.title(\"New Image\")\n",
    "plt.imshow(img_gray)\n",
    "plt.show()\n",
    "\n",
    "# compare to original MNIST image\n",
    "plt.title(\"Original MNIST\")\n",
    "plt.imshow(img.view(-1,28)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbMh8fqYTG1a"
   },
   "source": [
    "### Cropping the Image\n",
    "The images used to train our model were centered on the handwritten digit and resized to 28 x 28 pixesl. We will need to do the same to our new data in order for it work with our Multi-Class ANN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876
    },
    "id": "flRxtggCmRBX",
    "outputId": "fdedeb5b-1f9b-4841-f46f-11987a59f6d3"
   },
   "outputs": [],
   "source": [
    "#cropping\n",
    "img_cropped = img_gray[95:120,5:33]\n",
    "plt.title(\"Image Cropped\")\n",
    "plt.imshow(img_cropped)\n",
    "plt.show()\n",
    "\n",
    "#resize image\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "img_resized = resize(img_cropped, (28,28), anti_aliasing=True)\n",
    "\n",
    "#plot resized image\n",
    "plt.title(\"Image Resized\")\n",
    "plt.imshow(img_resized)\n",
    "plt.show()\n",
    "\n",
    "#image max and min values\n",
    "print(np.amax(img_resized))\n",
    "print(np.amin(img_resized))\n",
    "\n",
    "#normalize to range 0 to 1\n",
    "img_resized = img_resized / np.amax(img_resized)\n",
    "plt.title(\"Image Normalized\")\n",
    "plt.imshow(img_resized)\n",
    "plt.show()\n",
    "\n",
    "#verify max and min values\n",
    "print(np.amax(img_resized))\n",
    "print(np.amin(img_resized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngzTgm7JUoxk"
   },
   "source": [
    "If required, how could you invert the colours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzDHURiVU280"
   },
   "source": [
    "### Testing a New External Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "WqCkIOTFrfWv",
    "outputId": "1369cd7c-c672-4fc0-a73a-ee24459c6bc4"
   },
   "outputs": [],
   "source": [
    "#test new external image\n",
    "\n",
    "#plot resized image\n",
    "plt.title(\"New Image\")\n",
    "plt.imshow(img_resized)\n",
    "plt.show()\n",
    "\n",
    "#convert image to torch tensor\n",
    "img_new = torch.tensor(img_resized)\n",
    "print('Initial Dimensions: ', img_new.shape)\n",
    "\n",
    "#make our image match the model dimensions 1 x 28 x 28 and tensor type\n",
    "img_new = img_new.unsqueeze(0).type(torch.FloatTensor)\n",
    "print('Updated Dimensions: ', img_new.shape)\n",
    "\n",
    "#perform forward pass on ANN model and generate an output\n",
    "out = model(img_new)\n",
    "prob = F.softmax(out, dim=1)\n",
    "\n",
    "#examine output properties\n",
    "print('output dimensions: ', out.shape)\n",
    "print('output probabilities: ', prob, 'sum: ', torch.sum(prob))\n",
    "\n",
    "#print max index\n",
    "print('Predicted Output: ', prob.max(1, keepdim=True)[1].item(), 'with a probability of', prob.max(1, keepdim=True)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3XDF1ouU2hI"
   },
   "source": [
    "How did our Multi-Class ANN model perform? Was it successful on a new image?\n",
    "\n",
    "As a little exercise try to load new images and see if the model can classify them correctly. To start you can modify the code above to crop another portion of the image. Once you are comfortable with that, you can look for new images online, or make your own.\n",
    "\n",
    "**Congratulations! You have just completed a project for handwritten digit recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "039PX7SFZ0vm"
   },
   "source": [
    "## Tutorial Challenges\n",
    "There are just some questions for you to do on your own time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOO4yVXVxj11"
   },
   "source": [
    "### Classifications\n",
    "Obtain all classifications on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AoKeGxmxqiK"
   },
   "outputs": [],
   "source": [
    "#write code to obtain all predictions on the validation data set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeXyB34vaDuN"
   },
   "source": [
    "### Tutorial Challenge 1: Incorrect Classifications\n",
    "Go through all the predictions made on the validation dataset. How many digits were incorrectly classified? Are there any images that the ANN should have been able to classify correctly? View the images that were misclassified to speculate why they were misclassified. For example, you may find some **qualitative results** such as 2's are often mistaken as 3's or 1's are mistaken as 7's. Are there any samples that you would find difficult to classify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGuLiNHBarPt"
   },
   "outputs": [],
   "source": [
    "#write code to visualize some of the incorrectly classified images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gHy98YdwpVq"
   },
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix provide a nice table to visualize the classification performance of your model. Provided below is an example on a 6 sample toy dataset. In the final output the diagonal represents how many samples were correctly classified. For example: if we examine the third row of the result we will see that:\n",
    "\n",
    "- 1 sample was **incorrectly** classified as class 0\n",
    "- 0 samples were **incorrectly** classified as class 1\n",
    "- 2 samples were **correctly** classified as class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "IrPgb5w_wuZi",
    "outputId": "17124810-9009-4248-e967-65ae0fb385ae"
   },
   "outputs": [],
   "source": [
    "#Example - Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsF56vJAZXtM"
   },
   "source": [
    "### Tutorial Challenge 2: Confusion Matrix\n",
    "Continuing with our Multi-Class ANN problem. Obtain all the model predictions along with the labels (ground truths) and feed them into a confusion matrix. What insights can you obtain about the performance of your model? Are there certain digits that your model is better able to classify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSzA_lQtcRQo"
   },
   "outputs": [],
   "source": [
    "#Write code to obtain a confusion matrix of our multi-class ANN model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TUT_1B_ANN_Multiclass.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "adde43dfc76afd782a9f70772aad02055f592b1c305580742d2c68a703e29e52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
