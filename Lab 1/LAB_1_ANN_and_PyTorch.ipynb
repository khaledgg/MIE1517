{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0-skZWIG28h"
   },
   "source": [
    "# LAB 1: ANN and PyTorch [62 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6WDvajSqIDs"
   },
   "source": [
    "In this lab, you will start off by constructing a neural networks from scratch to get a stronger understanding of what is required to make neural networks work.\n",
    "\n",
    "The first part of the lab involves building a neural network from scratch to gain a better understanding of how they work. The second part of the lab involves training a neural network using PyTorch to classify images as either \"cat\" or \"dog\".  The code for the neural networks you train will be written for you, and you are not (yet!) expected\n",
    "to understand all provided code. However, by the end of the lab,\n",
    "you should be able to:\n",
    "\n",
    "1. Understand at a high level the training loop for a machine learning model.\n",
    "2. Understand the distinction between training, validation, and test data.\n",
    "3. The concepts of overfitting and underfitting.\n",
    "4. Investigate how different hyperparameters, such as learning rate and batch size, affect the success of training.\n",
    "5. Compare an ANN (aka Multi-Layer Perceptron) with a CNN.\n",
    "\n",
    "### What to submit\n",
    "\n",
    "Submit an HTML file containing all your code, outputs, and write-up\n",
    "from parts A and B. You can produce a HTML file directly from Google Colab. The Colab instructions are provided at the end of this document.\n",
    "\n",
    "**Do not submit any other files produced by your code.**\n",
    "\n",
    "Include a link to your colab file in your submission.\n",
    "\n",
    "Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfiFE_WOqIDu"
   },
   "source": [
    "## Colab Link\n",
    "\n",
    "Include a link to your colab file here\n",
    "\n",
    "Colab Link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXHk-DgnFpn7"
   },
   "source": [
    "# PART A: Constructing a Neural Network [15 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g4umO_YStI4"
   },
   "source": [
    "Before we get into using PyTorch to train our classifier we will go through the process of creating our neural network from scratch. We've seen in the tutorial how to build a 1-layer network, now we'll take it one step further to build a 2-layer network. This is an important exercise that everyone should attempt at least once to understand and truly appreciate the workings of neural networks.\n",
    "\n",
    "## Part A.0 Helper Functions\n",
    "\n",
    "To help guide the construction we will use the iris dataset that was introduced in the tutorial. Provided are some helper code to get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e2mMJ2cht4Lr"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# load \"Iris_3class.csv\" to Google Colab\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[0;32m      3\u001b[0m uploaded \u001b[39m=\u001b[39m files\u001b[39m.\u001b[39mupload()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# load \"Iris_3class.csv\" to Google Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wwlSwx8Rt-1v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_data = pd.read_csv(\"Iris_3class.csv\", header = None)\n",
    "raw_data = raw_data.values\n",
    "np.random.shuffle(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jAzKDXcWuE9k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (100, 1)\n",
      "float64 int32\n",
      "(50, 4) (50, 1)\n",
      "float64 int32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# raw_data = raw_data.values\n",
    "\n",
    "# split your data into training and validation\n",
    "X_train = raw_data[0:100,:4]\n",
    "y_train = raw_data[0:100,4:5].astype(int)\n",
    "X_val = raw_data[100:,:4]\n",
    "y_val = raw_data[100:,4:5].astype(int)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train.dtype, y_train.dtype)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_val.dtype, y_val.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbMT02fLuT-s"
   },
   "source": [
    "Recall that the neural network output consists of several nodes, one for each output class. Since the labels are provided as integers we will need to convert them into one-hot vectors to match the neural network output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fGVZ1NdTua7F"
   },
   "outputs": [],
   "source": [
    "#Convert array to one-hot encoding\n",
    "def to_one_hot(Y):\n",
    "    n_col = np.amax(Y) + 1\n",
    "    binarized = np.zeros((len(Y), n_col))\n",
    "    for i in range(len(Y)):\n",
    "        binarized[i, Y[i]] = 1.\n",
    "    return binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HqvHk7V0yLyY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (100, 3)\n",
      "float64 float64\n",
      "(50, 4) (50, 3)\n",
      "float64 float64\n"
     ]
    }
   ],
   "source": [
    "y_train = to_one_hot(y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train.dtype, y_train.dtype)\n",
    "\n",
    "y_val = to_one_hot(y_val)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_val.dtype, y_val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eQfZfIMqyPlT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify one-hot encoding\n",
    "y_train[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxHttgXHt2AM"
   },
   "source": [
    "## Part A.1 Develop a 2-layer ANN [5 pt]\n",
    "At its core a 2-layer neural network is just a few lines of code. Most of the complexity comes from setting up the training of the network.\n",
    "\n",
    "Using vectorized form, set up the neural network training to use a cross-entropy loss function and determine the gradients with resepect to the layer 1 and layer 2 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZifZzAqGCwt"
   },
   "outputs": [],
   "source": [
    "# write code to create a 2-layer ANN in vectorized form\n",
    "\n",
    "#define sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#define softmax\n",
    "def softmax(x):\n",
    "  e = np.exp(x)\n",
    "  return e/e.sum(axis=1, keepdims = True)\n",
    "\n",
    "\n",
    "def ann(W, X_train, y_train):\n",
    "\n",
    "  num_hidden = 5\n",
    "  num_features = 4\n",
    "  num_outputs = 3\n",
    "\n",
    "  #Weights\n",
    "  w0 = W[:20].reshape(num_features, num_hidden)\n",
    "  w1 = W[20:].reshape(num_hidden, num_outputs)\n",
    "\n",
    "  #Feed forward\n",
    "  layer0 = X_train\n",
    "  layer1 = sigmoid(np.dot(layer0, w0))\n",
    "  layer2 = np.dot(layer1, w1)\n",
    "  \n",
    "  # softmax\n",
    "  output = softmax(layer2)\n",
    " \n",
    "  #Back propagation using gradient descent\n",
    "  \n",
    "  #cross-entropy loss\n",
    "  error = # TO BE COMPLETED\n",
    "\n",
    "  #initialize gradients to zero \n",
    "  dw0 = # TO BE COMPLETED\n",
    "  dw1 = # TO BE COMPLETED\n",
    "\n",
    "  #calculate gradients\n",
    "  # TO BE COMPLETED\n",
    "\n",
    "  #determine gradients\n",
    "  dw1 += # TO BE COMPLETED\n",
    "  dw0 += # TO BE COMPLETED\n",
    "  \n",
    "  #combine gradients into one vector\n",
    "  dW = # TO BE COMPLETED\n",
    "\n",
    "  return (error, dW, layer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IGs9qYI38-8"
   },
   "source": [
    "## Part A.2 Train your neural network. How well does your network work on the iris dataset? [5 pt]\n",
    "\n",
    "Part A.2 of the lab assignment is focused on training the neural network that was created in Part 1. The code initializes the number of hidden units, features, and outputs. Then it initializes random weights for the two layers of the network, and combines them into a single vector W. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUJtiU_S38_G"
   },
   "outputs": [],
   "source": [
    "num_hidden = 5\n",
    "num_features = 4\n",
    "num_outputs = 3\n",
    "  \n",
    "#initialize weights\n",
    "w0 = 2*np.random.random((num_features, num_hidden)) - 1\n",
    "w1 = 2*np.random.random((num_hidden, num_outputs)) - 1\n",
    "\n",
    "#combine weights into a single vector\n",
    "W = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
    "\n",
    "#train network\n",
    "n = 0.001\n",
    "iterations = 100000\n",
    "errors = []\n",
    "for i in range(iterations):\n",
    "  (error, dW, y_pred) = ann(W, X_train, y_train)\n",
    "  W += -dW * n\n",
    "  errors.append(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUJconDT4IQ_"
   },
   "outputs": [],
   "source": [
    "#examine predictions on training data\n",
    "(_, _, y_pred) = ann(W, X_train, y_train)\n",
    "pred = np.round(y_pred, 0)\n",
    "pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine ground truth training data\n",
    "train = np.round(y_train, 0)\n",
    "train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O_Yh7d4rQJT"
   },
   "source": [
    "## Part A.3 Validate that the gradients were computed correctly for the 2-layer neural network you developed. [5 pt]\n",
    "\n",
    "Part A.3 is focused on validating the gradients computed for the 2-layer neural network. The code initializes random weights for the two layers of the network and combines them into a single vector W. It then computes the gradients analytically using the ann() function, and then computes the gradients numerically. The code does this by slightly incrementing the weights and computing the errors for each increment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQ6naP49rau-"
   },
   "outputs": [],
   "source": [
    "#write code to numerical verify the gradients you calculated\n",
    "\n",
    "num_hidden = 5\n",
    "num_features = 4\n",
    "num_outputs = 3\n",
    "  \n",
    "#initialize weights\n",
    "w0 = 2*np.random.random((num_features, num_hidden)) - 1\n",
    "w1 = 2*np.random.random((num_hidden, num_outputs)) - 1\n",
    "\n",
    "#combine weights\n",
    "W = # TO BE COMPLETED\n",
    "\n",
    "#compute gradients analytically\n",
    "(error, dW, y_pred) = ann(W, X_train, y_train)\n",
    "\n",
    "#compute gradients numerically\n",
    "dW_num = np.zeros((len(W),1))\n",
    "\n",
    "for ind in range(len(W)):\n",
    "  #reset gradients\n",
    "  We1 = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
    "  We2 = np.array(list(w0.flatten()) + list(w1.flatten()))\n",
    "  \n",
    "  #increment slightly\n",
    "  We1[ind] = # TO BE COMPLETED\n",
    "  We2[ind] = # TO BE COMPLETED\n",
    "  \n",
    "  #compute errors\n",
    "  (error_e1, dW_e1, y_pred1) = ann(We1, X_train, y_train)\n",
    "  (error_e2, dW_e2, y_pred2) = ann(We2, X_train, y_train)\n",
    "  \n",
    "  #obtain numerical gradients\n",
    "  grad_num = # TO BE COMPLETED\n",
    "  \n",
    "  #display difference between numerical and analytic gradients\n",
    "  print(round(abs(grad_num - dW[ind]), 4), grad_num, dW[ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82jjZkYJzLI9"
   },
   "source": [
    "# PART B: Cats vs Dogs [47 pt]\n",
    "\n",
    "In Part B of the lab, we will use PyTorch to train a neural network to identify cats and Dogs. The code starts by importing some libraries, including numpy, torch, and torchvision. This section provides some helper functions for loading and preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwyDuiuUqIDv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvTXpH_kqIDy"
   },
   "source": [
    "## Part B.0 Helper Functions\n",
    "\n",
    "We will be making use of the following helper functions. You will be asked to look\n",
    "at and possibly modify some of these, but you are not expected to understand all of them.\n",
    "\n",
    "You should look at the function names and read the docstrings. If you are curious, come back and explore the code *after* making some progress on the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEjwWEkoqIDz"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Data Loading\n",
    "\n",
    "def get_relevant_indices(dataset, classes, target_classes):\n",
    "    \"\"\" Return the indices for datapoints in the dataset that belongs to the\n",
    "    desired target classes, a subset of all possible classes.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset object\n",
    "        classes: A list of strings denoting the name of each class\n",
    "        target_classes: A list of strings denoting the name of desired classes\n",
    "                        Should be a subset of the 'classes'\n",
    "    Returns:\n",
    "        indices: list of indices that have labels corresponding to one of the\n",
    "                 target classes\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        # Check if the label is in the target classes\n",
    "        label_index = dataset[i][1] # ex: 9\n",
    "        label_class = classes[label_index] # ex: 'truck'\n",
    "        if label_class in target_classes:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def get_data_loader(target_classes, batch_size):\n",
    "    \"\"\" Loads images of cat and dogs, splits the data into training, validation\n",
    "    and testing datasets. Returns data loaders for the three preprocessed datasets.\n",
    "\n",
    "    Args:\n",
    "        target_classes: A list of strings denoting the name of the desired\n",
    "                        classes. Should be a subset of the argument 'classes'\n",
    "        batch_size: A int representing the number of samples per batch\n",
    "    \n",
    "    Returns:\n",
    "        train_loader: iterable training dataset organized according to batch size\n",
    "        val_loader: iterable validation dataset organized according to batch size\n",
    "        test_loader: iterable testing dataset organized according to batch size\n",
    "        classes: A list of strings denoting the name of each class\n",
    "    \"\"\"\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    ########################################################################\n",
    "    # The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "    # We transform them to Tensors of normalized range [-1, 1].\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    # Load CIFAR10 training data\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    # Get the list of indices to sample from\n",
    "    relevant_indices = get_relevant_indices(trainset, classes, target_classes)\n",
    "    \n",
    "    # Split into train and validation\n",
    "    np.random.seed(1000) # Fixed numpy random seed for reproducible shuffling\n",
    "    np.random.shuffle(relevant_indices)\n",
    "    split = int(len(relevant_indices) * 0.8) #split at 80%\n",
    "    \n",
    "    # split into training and validation indices\n",
    "    relevant_train_indices, relevant_val_indices = relevant_indices[:split], relevant_indices[split:]  \n",
    "    train_sampler = SubsetRandomSampler(relevant_train_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                               num_workers=1, sampler=train_sampler)\n",
    "    val_sampler = SubsetRandomSampler(relevant_val_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              num_workers=1, sampler=val_sampler)\n",
    "    # Load CIFAR10 testing data\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    # Get the list of indices to sample from\n",
    "    relevant_test_indices = get_relevant_indices(testset, classes, target_classes)\n",
    "    test_sampler = SubsetRandomSampler(relevant_test_indices)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, sampler=test_sampler)\n",
    "    return train_loader, val_loader, test_loader, classes\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def normalize_label(labels):\n",
    "    \"\"\"\n",
    "    Given a tensor containing 2 possible values, normalize this to 0/1\n",
    "\n",
    "    Args:\n",
    "        labels: a 1D tensor containing two possible scalar values\n",
    "    Returns:\n",
    "        A tensor normalize to 0/1 value\n",
    "    \"\"\"\n",
    "    max_val = torch.max(labels)\n",
    "    min_val = torch.min(labels)\n",
    "    norm_labels = (labels - min_val)/(max_val - min_val)\n",
    "    return norm_labels\n",
    "\n",
    "def evaluate(net, loader, criterion):\n",
    "    \"\"\" Evaluate the network on the validation set.\n",
    "\n",
    "     Args:\n",
    "         net: PyTorch neural network object\n",
    "         loader: PyTorch data loader for the validation set\n",
    "         criterion: The loss function\n",
    "     Returns:\n",
    "         err: A scalar for the avg classification error over the validation set\n",
    "         loss: A scalar for the average loss function over the validation set\n",
    "     \"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_epoch = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = normalize_label(labels)  # Convert labels to 0/1\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        corr = (outputs > 0.0).squeeze().long() != labels\n",
    "        total_err += int(corr.sum())\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "    err = float(total_err) / total_epoch\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    return err, loss\n",
    "\n",
    "###############################################################################\n",
    "# Training Curve\n",
    "def plot_training_curve(path):\n",
    "    \"\"\" Plots the training curve for a model run, given the csv files\n",
    "    containing the train/validation error/loss.\n",
    "\n",
    "    Args:\n",
    "        path: The base path of the csv files produced during training\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
    "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    n = len(train_err) # number of epochs\n",
    "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJxMgWGNqID2"
   },
   "source": [
    "## Part B.1 Visualizing the Data [5 pt]\n",
    "\n",
    "We will make use of some of the CIFAR-10 data set, which consists of \n",
    "colour images of size 32x32 pixels belonging to 10 categories. You can\n",
    "find out more about the dataset at https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "For this assignment, we will only be using the cat and dog categories. \n",
    "We have included code that automatically downloads the dataset the \n",
    "first time that the main script is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rp7LVcGfqID3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will download the CIFAR-10 dataset to a folder called \"data\"\n",
    "# the first time you run this code.\n",
    "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
    "    target_classes=[\"cat\", \"dog\"], \n",
    "    batch_size=1) # One image per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiYX_HM-qID6"
   },
   "source": [
    "### Part B.1.a - [1 pt]\n",
    "\n",
    "Visualize some of the data by running the code below.\n",
    "Include the visualization in your writeup.\n",
    "\n",
    "Your figures should show up directly in the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ4pYFTQqID7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k = 0\n",
    "for images, labels in train_loader:\n",
    "    # since batch_size = 1, there is only 1 image in `images`\n",
    "    image = images[0]\n",
    "    # place the colour channel at the end, instead of at the beginning\n",
    "    img = np.transpose(image, [1,2,0])\n",
    "    # normalize pixel intensity values to [0, 1]\n",
    "    img = img / 2 + 0.5\n",
    "    plt.subplot(3, 5, k+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    k += 1\n",
    "    if k > 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmHiUOaDqID9"
   },
   "source": [
    "### Part B.1.b - [2 pt]\n",
    "\n",
    "How many training examples do we have for the combined `cat` and `dog` classes? \n",
    "What about validation examples? \n",
    "What about test examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6p4eAz1IqID-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aY25I_oqIEB"
   },
   "source": [
    "### Part B.1.c - [2pt]\n",
    "\n",
    "Why do we need a validation set when training our model? What happens if we judge the \n",
    "performance of our models using the training set loss/error instead of the validation\n",
    "set loss/error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYqVAG5YqIEB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iYZ4xXHqIEE"
   },
   "source": [
    "## Part B.2 Training [10 pt]\n",
    "\n",
    "We define two neural networks, a `LargeNet` and `SmallNet`.\n",
    "We'll be training the networks in this section.\n",
    "\n",
    "You won't understand fully what these networks are doing until\n",
    "the next few classes, and that's okay. For this assignment, please\n",
    "focus on learning how to train networks, and how hyperparameters affect\n",
    "training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HniZoI3lqIEF"
   },
   "outputs": [],
   "source": [
    "class LargeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNet, self).__init__()\n",
    "        self.name = \"large\"\n",
    "        self.conv1 = nn.Conv2d(3, 5, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
    "        self.fc1 = nn.Linear(10 * 5 * 5, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.squeeze(1) # Flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Qst9J6VM0ib"
   },
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.name = \"small\"\n",
    "        self.conv = nn.Conv2d(3, 5, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(5 * 7 * 7, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 5 * 7 * 7)\n",
    "        x = self.fc(x)\n",
    "        x = x.squeeze(1) # Flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMJZt_S2qIEM"
   },
   "outputs": [],
   "source": [
    "small_net = SmallNet()\n",
    "large_net = LargeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ekO3pj9qIEQ"
   },
   "source": [
    "### Part B.2.a - [1pt]\n",
    "\n",
    "The methods `small_net.parameters()` and `large_net.parameters()`\n",
    "produces an iterator of all the trainable parameters of the network.\n",
    "These parameters are torch tensors containing many scalar values. \n",
    "\n",
    "We haven't learned how how the parameters in these high-dimensional\n",
    "tensors will be used, but we should be able to count the number\n",
    "of parameters. Measuring the number of parameters in a network is\n",
    "one way of measuring the \"size\" of a network.\n",
    "\n",
    "What is the total number of parameters in `small_net` and in\n",
    "`large_net`? (Hint: how many numbers are in each tensor?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc4keyOnqIER"
   },
   "outputs": [],
   "source": [
    "for param in small_net.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcZVo2DsqIET"
   },
   "source": [
    "### The function train_net\n",
    "\n",
    "The function `train_net` below takes an untrained neural network (like `small_net` and `large_net`) and\n",
    "several other parameters. You should be able to understand how this function works.\n",
    "The figure below shows the high level training loop for a machine learning model:\n",
    "\n",
    "![alt text](https://github.com/UTNeural/Lab2/blob/master/Diagram.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC90G_VGqIEU"
   },
   "outputs": [],
   "source": [
    "def train_net(net, batch_size=64, learning_rate=0.01, num_epochs=30):\n",
    "    ########################################################################\n",
    "    # Train a classifier on cats and dogs\n",
    "    target_classes = [\"cat\", \"dog\"]\n",
    "    ########################################################################\n",
    "    # Fixed PyTorch random seed for reproducible result\n",
    "    torch.manual_seed(1000)\n",
    "    ########################################################################\n",
    "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
    "    train_loader, val_loader, test_loader, classes = get_data_loader(\n",
    "            target_classes, batch_size)\n",
    "    ########################################################################\n",
    "    # Define the Loss function and optimizer\n",
    "    # The loss function will be Binary Cross Entropy (BCE). In this case we\n",
    "    # will use the BCEWithLogitsLoss which takes unnormalized output from\n",
    "    # the neural network and scalar label.\n",
    "    # Optimizer will be SGD with Momentum.\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    ########################################################################\n",
    "    # Set up some numpy arrays to store the training/test loss/erruracy\n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    ########################################################################\n",
    "    # Train the network\n",
    "    # Loop over the data iterator and sample a new batch of training data\n",
    "    # Get the output from the network, and optimize our loss function.\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "        total_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Get the inputs\n",
    "            inputs, labels = data\n",
    "            labels = normalize_label(labels) # Convert labels to 0/1\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass, backward pass, and optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Calculate the statistics\n",
    "            corr = (outputs > 0.0).squeeze().long() != labels\n",
    "            total_train_err += int(corr.sum())\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\n",
    "               \"Validation err: {}, Validation loss: {}\").format(\n",
    "                   epoch + 1,\n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]))\n",
    "        # Save the current model (checkpoint) to a file\n",
    "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEhligNaqIEW"
   },
   "source": [
    "### Part B.2.b - [1pt]\n",
    "\n",
    "The parameters to the function `train_net` are hyperparameters of our neural network.\n",
    "We made these hyperparameters easy to modify so that we can tune them later on. \n",
    "\n",
    "What are the default values of the parameters `batch_size`, `learning_rate`, \n",
    "and `num_epochs`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybq2F5KTqIEX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy_yv9NMqIEa"
   },
   "source": [
    "### Part B.2.c - [1 pt]\n",
    "\n",
    "What files stored in memory when we call `train_net` with `small_net`, and train for 5 epochs? Provide a list\n",
    "of all the files written to disk, and what information the files contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNwhtyP1qIEb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6shtCFDqIEd"
   },
   "source": [
    "### Part B.2.d - [2pt]\n",
    "\n",
    "Train both `small_net` and `large_net` using the function `train_net` and its default parameters.\n",
    "The function will write many files to disk, including a model checkpoint (saved values of model weights) \n",
    "at the end of each epoch.\n",
    "\n",
    "If you are using Google Colab, you will need to mount Google Drive \n",
    "so that the files generated by `train_net` gets saved. We will be using\n",
    "these files in part (d).\n",
    "(See the Google Colab tutorial for more information about this.)\n",
    "\n",
    "Report the total time elapsed when training each network. Which network took longer to train?\n",
    "Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tawIVwTfqIEe"
   },
   "outputs": [],
   "source": [
    "# Since the function writes files to disk, you will need to mount\n",
    "# your Google Drive. If you are working on the lab locally, you\n",
    "# can comment out this code.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWSZm9SvqIEh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk4GIF3QqIEj"
   },
   "source": [
    "### Part B.2.e - [2pt]\n",
    "\n",
    "Use the function `plot_training_curve` to display the trajectory of the \n",
    "training/validation error and the training/validation loss.\n",
    "You will need to use the function `get_model_name` to generate the\n",
    "argument to the `plot_training_curve` function.\n",
    "\n",
    "Do this for both the small network and the large network. Include both plots\n",
    "in your writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXrEgOtuqIEk"
   },
   "outputs": [],
   "source": [
    "#model_path = get_model_name(\"small\", batch_size=??, learning_rate=??, epoch=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUzWo8HRqIEu"
   },
   "source": [
    "### Part B.2.f - [3pt]\n",
    "\n",
    "Describe what you notice about the training curve.\n",
    "How do the curves differ for `small_net` and `large_net`?\n",
    "Identify any occurences of underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqlu1fHZqIEw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYP_y58SqIEz"
   },
   "source": [
    "## Part B.3 Optimization Parameters [4 pt]\n",
    "\n",
    "For this section, we will work with `large_net` only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1uGdKUIqIE1"
   },
   "source": [
    "### Part B.3.a - [1pt]\n",
    "\n",
    "Train `large_net` with all default parameters, except set `learning_rate=0.001`.\n",
    "Does the model take longer/shorter to train?\n",
    "Plot the training curve. Describe the effect of *lowering* the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9-toERSqIE2"
   },
   "outputs": [],
   "source": [
    "# Note: When we re-construct the model, we start the training\n",
    "# with *random weights*. If we omit this code, the values of\n",
    "# the weights will still be the previously trained values.\n",
    "large_net = LargeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5APYluUoqIE4"
   },
   "source": [
    "### Part B.3.b - [1pt]\n",
    "\n",
    "Train `large_net` with all default parameters, except set `learning_rate=0.1`. \n",
    "Does the model take longer/shorter to train?\n",
    "Plot the training curve. Describe the effect of *increasing* the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY7GrjFxqIE5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMb-NLitqIE7"
   },
   "source": [
    "### Part B.3.c - [1pt]\n",
    "\n",
    "Train `large_net` with all default parameters, including with `learning_rate=0.01`.\n",
    "Now, set `batch_size=512`. Does the model take longer/shorter to train?\n",
    "Plot the training curve. Describe the effect of *increasing* the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHRzqMAcqIE8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwvN15IzqIE-"
   },
   "source": [
    "### Part B.3.d - [1pt]\n",
    "\n",
    "Train `large_net` with all default parameters, including with `learning_rate=0.01`.\n",
    "Now, set `batch_size=16`. Does the model take longer/shorter to train?\n",
    "Plot the training curve. Describe the effect of *decreasing* the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0SKvrNGqIE-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MkpZLc4qIFA"
   },
   "source": [
    "## Part B.4 Hyperparameter Search [4 pt]\n",
    "\n",
    "### Part B.4.a - [1pt]\n",
    "\n",
    "Based on the plots from above, choose another set of values for the hyperparameters (network, batch_size, learning_rate)\n",
    "that you think would help you improve the validation accuracy. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MocaZsUqIFB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTLRs2f7qIFD"
   },
   "source": [
    "### Part B.4.b - [1pt]\n",
    "\n",
    "Train the model with the hyperparameters you chose in part(a), and include the training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCFV10I9qIFD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsO4z7JJqIFF"
   },
   "source": [
    "### Part B.4.c - [1pt]\n",
    "\n",
    "Based on your result from Part(a), suggest another set of hyperparameter values to try. \n",
    "Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sZjOR2RqIFG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrMyc5QnqIFI"
   },
   "source": [
    "### Part B.4.d - [1pt]\n",
    "\n",
    "Train the model with the hyperparameters you chose in part(c), and include the training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AU0a0PHFqIFJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kQB1IFuqIFN"
   },
   "source": [
    "## Part B.5 Evaluating the Best Model [9 pt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxje12xKqIFN"
   },
   "source": [
    "### Part B.5.a - [1pt]\n",
    "\n",
    "Choose the **best** model that you have so far. This means choosing the best model checkpoint,\n",
    "including the choice of `small_net` vs `large_net`, the `batch_size`, `learning_rate`, \n",
    "**and the epoch number**.\n",
    "\n",
    "Modify the code below to load your chosen set of weights to the model object `net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AanLwIAEqIFO"
   },
   "outputs": [],
   "source": [
    "net = SmallNet()\n",
    "model_path = get_model_name(net.name, batch_size=64, learning_rate=0.01, epoch=10)\n",
    "state = torch.load(model_path)\n",
    "net.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psNcmnIPqIFQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXXwLMIXqIFS"
   },
   "source": [
    "### Part B.5.b - [2pt]\n",
    "\n",
    "Justify your choice of model from part (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY0cG56EqIFS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0GE8Z5kqIFU"
   },
   "source": [
    "### Part B.5.c - [2pt]\n",
    "\n",
    "Using the code in Part 0, any code from lecture notes, or any code that you write,\n",
    "compute and report the **test classification error** for your chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPUe9iomqIFV"
   },
   "outputs": [],
   "source": [
    "# If you use the `evaluate` function provided in part 0, you will need to \n",
    "# set batch_size > 1\n",
    "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
    "    target_classes=[\"cat\", \"dog\"], \n",
    "    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXK0XaayqIFY"
   },
   "source": [
    "### Part B.5.d - [2pt]\n",
    "\n",
    "How does the test classification error compare with the **validation error**?\n",
    "Explain why you would expect the test error to be *higher* than the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxcr2b9NqIFZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMA9b8vXqIFd"
   },
   "source": [
    "### Part B.5.e - [2pt]\n",
    "\n",
    "Why did we only use the test data set at the very end?\n",
    "Why is it important that we use the test data as little as possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPH4pTRKqIFe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxG0sYDZM2KR"
   },
   "source": [
    "## Part B.6 Fully-Connected Linear ANN vs CNN [9 pt]\n",
    "\n",
    "Test out a 3-layer linear fully-connected ANN architecture (see simpleANN below). You should explore different hyperparameter settings to determine how well you can do on the validation dataset. Once satisified with the performance, you may test it out on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.6.a - [3 pt]\n",
    "\n",
    "How does the your best CNN model compare with an 2-layer linear ANN model (no convolutional layers) on classifying cat and dog images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3N9qs5xOvSo"
   },
   "outputs": [],
   "source": [
    "class simpleANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleANN, self).__init__()\n",
    "        self.name = \"simple\"\n",
    "        self.fc1 = nn.Linear(32*32*3, 100)\n",
    "        self.fc2 = nn.Linear(100, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.squeeze(1) # Flatten to [batch_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSDohNkBOlZf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.6.b - [6 pt]\n",
    "\n",
    "Implement a 3-layer ANN architecture with dropout and batch normalization (separately). Compare the performance of this model with the simpleANN model provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform your comparison here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B.7-Conceptual Questions [6 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.7.a - [3 pt]\n",
    "\n",
    "Explain the difference between L1 and L2 regularization and when it is appropriate to use one over the other. How would you implement L1 regularization in simpleANN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.7.b - [3 pt]\n",
    "\n",
    "Describe the impact of different activation functions (e.g. ReLU, Sigmoid, Tanh) on the performance of the model. How would you go about choosing the appropriate activation function for simpleANN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYwI4RmFS2RB"
   },
   "source": [
    "### Saving to HTML\n",
    "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
    "\n",
    "(1) download your ipynb file by clicking on File->Download.ipynb\n",
    "\n",
    "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
    "\n",
    "(3) run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TrsqdNgS5ex"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "jupyter nbconvert --to html LAB_1_ANN_and_PyTorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuXhlFlPTY7F"
   },
   "source": [
    "(4) the html file will be available for download in the temporary Google Colab storage\n",
    "\n",
    "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LAB_1_ANN_and_PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "adde43dfc76afd782a9f70772aad02055f592b1c305580742d2c68a703e29e52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
